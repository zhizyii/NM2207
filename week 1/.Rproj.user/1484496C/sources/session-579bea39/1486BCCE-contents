---
title: "DSA3361 - Workshop 1"
date: '2023'
output:
  html_document: default
  pdf_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, out.width='80%', fig.align='center')

```

# Structure of Workshop (mini-lecture)

1. Sampling distributions of test statistics/point estimates
2. Bootstrap confidence intervals
3. Permutation tests

# Learning Outcomes

We will learn to:

1. Generate the sampling distribution for some point estimate.
2. Construct the Bootstrap confidence intervals.
3. Apply permutation tests.

### Setting Up Rstudio

Here's a checklist:

1. Have you created an Rstudio project for this class?
    * Create a new folder `DSA3361` and create a new Rstudio project inside it.
2. Have you downloaded all the data, source codes and slides from Canvas?
    * Create `data`, `src` and `slides` folders within the main folder and dump the Canvas contents there.

## Installing and Using Libraries

Libraries extend the functionality of R. They are essentially a collection of 
functions. Before **using** a library, we need to load it.

These are the libraries we need for today:

```{r load_libraries}
library(pacman)
p_load(ggplot2, dplyr, stats, fitdistrplus, vcd, fGarch, ufs, tinytex)

```

If you encounter an error such as this, it means that the library has not been installed yet.

```
Error in library(fitdistrplus) : there is no package called ‘fitdistrplus’
```

# Task 1: Sampling Distributions and Non-parametric Boostrap CIs

In the videos, we discussed that the classical hypothesis testing, including p-values and confidence intervals (CIs), is a parametric statistical test, which makes assumptions about the underlying population distribution from which the sample data is drawn.

For example, such assumptions may include normality, constant variance, and independence of observations.

If the assumptions are met, the approach is effective in the sense that the sampling distribution(s) can be described rigorously. The statistical inference could be reliable. 

However, in the absence of the assumptions, which is not rare in practice, the accuracy of p-values or CIs may suffer and the inferences may not be reliable. 

Also, the classical approach limits the possible types of statistics that we could use.

The good news is that permutation testing and bootstrapping testing are alternative solutions. Their main idea is to **\textcolor{red}{simulate}** the sampling distribution, which requires less assumptions.  For example, we do not require the sample size to be large, and we do not require that the underlying population data follow a normal distribution. Another key benefit is that we can (almost) use or design any test statistic that is reasonable. A test statistic is also referred to as a point estimate.  

The core of the non-parametric bootstrapping and permutation testing is the re-sampling method. Both approaches repeatedly resample from the original sample. The difference is that the non-parametric bootstrapping approach is to resample with replacement while the permutation test approach is to resample without replacement. 

We will use the following example to illustrate the concept of point estimates, and generate the sampling distributions. 

## 1-1: Case Study: E&E Courses Fees Data

Some unit of NUS offers Executive Education courses that offer in-demand skills and knowledge according to technological and business trends. We have obtained data from past courses. We are interested to study the spending of our learners, and calculate the mean spending of each discipline.

Let us first import the dataset. 

```{r}
# If your working directory is "***/DSA3361/src" or "***/DSA3361/code",
df_fees <- read.csv("NUS_E&E_fees.csv") 
df_fees <- read.csv("../data/NUS_E&E_fees.csv") 
str(df_fees)
head(df_fees)
summary(df_fees)
```

Let us inspect the dataset by checking how many disciplines are involved. 

```{r}
df_fees$discipline <- as.factor(df_fees$discipline) 

str(df_fees)
levels(df_fees$discipline)
summary(df_fees)
```

This dataset involves 3 disciplines: `Artificial Intelligence`, `Data Science` and `StartUp & Tech Development`. 

> Question: Which disciplines are more popular, with regards to the number of participants?

## 1-2: Point Estimates

It is natural to use the sample mean (spending) as a point estimate for each discipline. 

```{r}
# compare the mean fees of the three disciplines
(mean_AI <- mean(df_fees[df_fees$discipline == "Artificial Intelligence",2]))
(mean_DS <- mean(df_fees[df_fees$discipline == "Data Science",2]))
(mean_STD <- mean(df_fees[df_fees$discipline == "StartUp & Tech Development",2]))

# alternative way of comparing the mean fees
tapply(df_fees$fees, df_fees$discipline, mean)

# Hands-on: Calculate the standard deviations (SD) for the fees of the three groups
tapply(df_fees$fees, df_fees$discipline, sd)

boxplot(fees ~ discipline, data = df_fees)
```

Our ultimate target is to infer the population parameter, namely, the population mean spending of each discipline. The following discussion will focus on AI for convenience. 

```{r}
fees_AI <- df_fees[df_fees$discipline == "Artificial Intelligence",2]
hist(fees_AI)
```

You may check the distribution types of course fees of other disciplines. 

Alternatively, we can use ggplot() to inspect the four histograms simultaneously.   

```{r}
# Y axis is count:
ggplot(df_fees, aes(x = fees, fill = discipline)) + geom_histogram() +
      facet_wrap(~discipline)

# Y axis is density:
ggplot(df_fees, aes(x = fees, fill = discipline)) + geom_histogram(aes(y=after_stat(density))) +
      facet_wrap(~discipline)
```

> What do you observe?

## 1-3: Sampling Distribution

In order to make reliable inferences on the population parameters, we wish to study the sampling distribution of the test statistic, the mean course fees. Recall that a sampling distribution is the distribution of all possible values of test statistics of random samples of a given size from a given population. 

The sampling distribution describes how a sample statistic (e.g., sample mean) varies from one study to another, while the population distribution describes the variability of individual observations.

The following is a simple example of how sampling distribution could be generated, if the population data is known. 

```{r}
set.seed(1234)
pop_income <- rsnorm(10000, mean=10000, sd=3000, xi=2)  # population data
hist(pop_income, main = "Population Distribution of Household Income")  
# set the population parameter, the mean household income
para_pop <- 10000

n_rep <- 1000  # number of random samples
sample_size <- 30
sampling_dist <- numeric(n_rep)

set.seed(1)
for (i in 1:n_rep){
  sample1 <- sample(pop_income, size = sample_size, replace = FALSE)
  sampling_dist[i] <- mean(sample1)
}

hist(sampling_dist, main = "Sampling Distribution of Sample Mean")
mean(sampling_dist)  
mean(sampling_dist) - para_pop   # the variability due to the simulation, which could be reduced with more samples 
```

> Question: how do we reduce the above variability?

The above histogram is not the exact sampling distribution but an approximation. The larger the number of random samples (i.e., n_rep but not the sample size), the closer the histogram will be to the exact one. 

## 1-4: Boostrap Distribution

The following is to apply the non-parametric bootstrap method to generate the sampling distribution of sample mean for AI. We will refer to the distribution as the bootstrap distribution of means.  

```{r}
n_rep <- 1000
boot_fees_AI <- numeric(n_rep)

set.seed(1)
for (i in 1:n_rep){
  boot_sample <- sample(fees_AI, size = length(fees_AI), replace = TRUE)
  boot_fees_AI[i] <- mean(boot_sample)
}

hist(boot_fees_AI,
     main = "NP Bootstrap distribution of mean (Fees for AI)")
abline(v=mean(fees_AI), lty = 2, lwd = 3, col="red")

mean(boot_fees_AI)                  #mean X-bar*
sd(boot_fees_AI)                    #SE*

mean(fees_AI)

mean(boot_fees_AI) - mean(fees_AI)  
```

> How do we reduce such discrepancy? What is your recommendation?

Note that the (simulated) bootstrap distribution is centered around the original sample mean, which is not necessarily close to the unknown population parameter. Therefore, the bootstrap method is not used to get better estimates of the parameter value; instead, it is useful for quantifying the variability of a parameter estimate, such as, standard error, skewness, or for calculating confidence intervals. 

## 1-5: Compute and Plot the 95% Confidence Intervals

The general template of confidence interval is as follows:

\begin{equation*} 
				\begin{split}
				\textsf{Confidence Interval} & = \textsf{Point Estimate} \pm \textsf{Margin of Error}  \\
 				& = \textsf{Point Estimate} \pm \textsf{Critical Value} * \textsf{Standard Error}
				\end{split}
\end{equation*}

We first construct the classical SE (standard error method) confidence interval, which shares the same set of assumptions as one sample t-test.

```{r}
mean(fees_AI)
sd(fees_AI)
(ct_value <- qt(0.975, df = length(fees_AI) - 1))
(moe <- ct_value * sd(fees_AI)/sqrt(length(fees_AI)))

ci.lb <- mean(fees_AI) - moe
ci.ub <- mean(fees_AI) + moe

# classical SE CI
ci_classical <- formatCI(c(ci.lb,ci.ub))
print(paste0("The 95% classical SE CI for mean is ", ci_classical))
```

For the classical approach, standard error is typically calculated using the sample size(s) and the sample (or population) standard deviation(s). 

We next construct the bootstrap CI using the standard error method, where standard error is equal to standard deviation of the collection of the bootstrap sample statistics. This is also referred to as **Bootstrap SE Confidence Interval**.

We first assume the bootstrap distribution is normal; hence, we set the critical value as the normal-based one, 1.96.   

```{r}
mean(boot_fees_AI)
sd(boot_fees_AI)
moe <- 1.96 * sd(boot_fees_AI)

ci.lb2 <- mean(boot_fees_AI) - moe
ci.ub2 <- mean(boot_fees_AI) + moe

# Bootstrap SE (standard error) CI
(ci_boot_se <- formatCI(c(ci.lb2,ci.ub2)))
print(paste0("The 95% Bootstrap SE CI for mean is ", ci_boot_se))
```

We next evaluate whether the bootstrap distribution is normal. 

```{r}
fit_norm_object <- fitdist(boot_fees_AI, distr = "norm")
qqcomp(fit_norm_object, main = "Bootstrap Distribution")
denscomp(fit_norm_object)
```

The bootstrap distribution (mean of AI group) is slightly right skewed. The normality assumption here is possibly violated. Let us double check by calculating the percentage of data below and above the CI range. 

```{r}
percent_below <- mean(boot_fees_AI < ci.lb2)
percent_above <- mean(boot_fees_AI > ci.ub2)

print(paste0("The proportion below the lower limit CI is ",percent_below))   
print(paste0("The proportion above the upper limit CI is ",percent_above))            
```

We conclude that 

- about 2.0% of the resample means are below the bootstrap mean -1.96SE;
- about 2.7% of the resample means are above the bootstrap mean +1.96SE.

In this case, relying on the CLT would be less accurate. 

Alternatively, we can construct 95% Bootstrap CI for the mean, using the **Percentile** method, i.e., 95% **Bootstrap Percentile Confidence Interval**.

```{r}
ci_boot_perc <- quantile(boot_fees_AI, c(0.025, 0.975))

print(paste0("The 95% Bootstrap Percentile CI for mean is ", formatCI(ci_boot_perc)))
```


```{r}
hist(boot_fees_AI)
abline(v=ci_boot_perc[1], lty = 2, lwd = 3, col="red")
abline(v=ci_boot_perc[2], lty = 2, lwd = 3, col="red")

(percent_below <- mean(boot_fees_AI < ci_boot_perc[1]))
(percent_above <- mean(boot_fees_AI > ci_boot_perc[2]))

# center
mu <- mean(boot_fees_AI)
# left margin 
mu - ci_boot_perc[1]
# right margin
ci_boot_perc[2] - mu
```

In conclusion: 

- A good confidence interval for the mean need not be symmetrical.
- When the bootstrap distribution is right skewed, the interval will likely stretch far to the right.  
- Conversely, there is less risk of missing the true mean on the lower side, so the left endpoint need not be as far away from the sample mean. 

> Optional: You could change the above code to calculate the 95% bootstrap percentile confidence interval for another statistic, median.

### Plot the Confidence Intervals

Let us plot the above calculated 95% confidence intervals, using the `ggplot()` function.  

```{r}
x <- c(1:3)
y <- c(mean(fees_AI), mean(boot_fees_AI), mean(boot_fees_AI))
lower_bound <- c(ci.lb, ci.lb2, as.numeric(ci_boot_perc[1]))
upper_bound <- c(ci.ub, ci.ub2, as.numeric(ci_boot_perc[2]))

# Create a data frame to hold the values
data_df <- data.frame(x = factor(x, levels = c(1, 2, 3)),
                      y = y,
                      lower_bound = lower_bound,
                      upper_bound = upper_bound)

ggplot(data_df, aes(x, y)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound), width = 0.1, position = position_dodge(0.1)) +
  ylim(c(3000, 5000)) +
  theme(axis.text.x = element_text(size = 15)) +
  geom_hline(yintercept = 4640, linetype = 2, linewidth = 1, color = "red") +
  geom_hline(yintercept = 3338.6, linetype = 2, linewidth = 1, color = "red") +
  scale_x_discrete(labels = c("Classical", "Bootstrap SE", "Bootstrap Percentile"))

```

When the sample size is large, the difference between classical, bootstrap SE and bootstrap Percentile confidence intervals is likely minimal. 

The bootstrap method is effective for cases where sample size is small, or the test statistic is uncommon, say, ratio of two sample variances.

We can also visualise the confidence intervals of the three subjects. 

For convenience, we will only calculate the classical normal-based confidence intervals. 

```{r}
# calculate the mean, sd and sample size for each discipline
discipline_fee_means <- group_by(df_fees, discipline) %>%
                        summarise(av_fee= mean(fees, na.rm = TRUE),
                                  sd_fee = sd(fees, na.rm = TRUE),
                                  n= n(), .groups="drop") %>%
                        mutate(ylower= av_fee - 1.96*sd_fee/sqrt(n),
                               yupper = av_fee + 1.96*sd_fee/sqrt(n)) 

ggplot(discipline_fee_means, aes(x=discipline, y=av_fee)) +
  geom_point(col="red", size=3) +
  geom_errorbar(aes(ymin=ylower, ymax=yupper), linewidth=1, width=0.2) +
  labs(x = "Discipline", y = "Average Fee") +
  theme(axis.text = element_text(size = 12)) 
```

### Summary

1. Bootstrap is very powerful in situations when we wish to find standard errors, perform hypothesis tests, and compute confidence intervals for new statistics/point estimators. (Median, ratio of two means/medians, ratio of two variances, etc.)

# Task 2: Permutation Testing

The core of the permutation tests is the re-sampling method. The permutation test approach is to re-sample from the original sample without replacement. Let us use the course fees data to practice. 

## 2-1: ANOVA Test

We are interested to determine whether the discipline is associated with the course fees. To compare the mean fees of the three disciplines, we can conduct an ANOVA test.

1. Null hypothesis: the three disciplines&apos; mean fees are the same.
2. Alternative hypothesis: the three disciplines&apos; mean fees are different.

```{r}
one.way <- aov(fees ~ discipline, data = df_fees)
summary(one.way)
```

The p-value from the ANOVA test is 0.000792. 

### Check the Assumptions

There are four assumptions for ANOVA test:

1. Each sample group is drawn from a normally distributed population.
2. All sample groups are randomly selected and independent.
3. Variance across groups are the same.
4. The residuals are normally distributed for each population.

```{r}
boxplot(fees ~ discipline, data = df_fees)
```

From the boxplot, the three populations are likely skewed. 

Therefore, not all the assumptions are satisfied. However, we don't know how much it may affect the accuracy of the p-value. 

### Hands-on (Optional)

As the normality assumption does not hold, we can also apply the non-parametric version of ANOVA test, Kruskal-Wallis H test.

```{r}
kruskal.test(fees ~ discipline, data = df_fees) # p-value = 0.000104
```

## 2-2: Permutation Testing

Recall the key steps of implementing a permutation test:

0. Define or construct a test statistic.
1. Define a function, which generates a permutation sample and calculates the test statistic of the permutation sample.
2. Use the `replicate()` function to repeat the above simulation multiple times, say, 10,000.  
3. Generate a histogram of the 10,000 test statistics that approximates the exact permutation distribution.
4. Calculate the p-value as the proportion of the test statistics that are at least as extreme as the observed one.

### Define a Test Statistic

The first thought is to use the original F value used in ANOVA test.

\[
F = \frac{\textsf{Mean Sum of Squares between the Groups}}{\textsf{Mean Sum of Residual Squares}}
\]

However, the mathematical form is a bit complicated and computationally expensive. We can fetch the F value from the summary of the ANOVA test table.

```{r}
# to fetch the sample's test statistic: F value
summary(one.way)[[1]]$`F value`[1]
```

One of the advantages of permutation testing is that you can almost define any reasonable test statistic that you want. 

In our case, we can define the test statistic as the sum of the absolute differences of the three groups' means. 

\[
\textsf{Test Statistic} = |\textsf{Mean}_1 - \textsf{Mean}_2| + |\textsf{Mean}_2 - \textsf{Mean}_3| + |\textsf{Mean}_3 - \textsf{Mean}_1|
\]

Let us calculate the new test statistic in the following way.

```{r}
(mean_list <- tapply(df_fees$fees, df_fees$discipline, mean))
a <- mean_list[1]
b <- mean_list[2]
c <- mean_list[3]
(original <- as.numeric(abs(a-b) + abs(a-c) + abs(b-c)))
```

### Define a Function to Simulate One Permutation Sample

```{r}
# Some prepartions
treatment <- df_fees$discipline
outcome <- df_fees$fees

permutation.test <- function(treatment, outcome){
    # Generate a permutation sample by reallocating the group order; it is equivalent to re-sampling without replacement
    treatment_p <- sample(treatment, size = length(treatment), replace = FALSE)
    
    # Calculate the test statistic for the permutation sample
    df_p <- data.frame(discipline = treatment_p, fees = outcome)
    mean_p <- tapply(df_p$fees, df_p$discipline, mean)
    a <- mean_p[1]
    b <- mean_p[2]
    c <- mean_p[3]
    as.numeric(abs(a-b) + abs(b-c) + abs(c-a))
}
```

### Generate Multiple Permutation Samples and the Permutation Distribution

Let us use the `replicate()` function to run the simulations 10,000 times.

```{r}
set.seed(123) 
test <- replicate(10000, permutation.test(treatment, outcome))

hist(test, main = "Sampling Distribution by Permutation") # Generate the histogram of all the 10,000 test statistics
abline(v=original, lty = 2, lwd = 3, col="red") # Mark the original sample's test statistic 

mean(abs(test)) >= abs(original) # p-value is _____
```

### Hands-on (Optional)

1. You can increase the number of simulations to see how the permutation distribution and the p-value change. 
2. You can remove the `set.seed` part, and run the remaining lines repeatedly to see how the chart and the p-value are updated. 
3. Do you have any other suggestions for the possible test statistic (for this ANOVA test)?

# Task 3: Fitting a Discrete Variable

To fit a discrete variable, there are three steps:  (using Poisson as an example)

1. fit_object <- fitdist(data = _____, distr = "pois")
2. fitted <- dpois(x = *support*, lambda = parameter) * sum(observed)
3. rootogram(x = observed, fitted = fitted)

## 3-1: Case Study: Customers' Arrival

Suppose that we wait at the bank's entrance, and note down the arrival time of the first 23 customers, which has been collated into the "customers.csv". 

We can naturally assume that one customer's arrival does not affect when the next customer will arrive, namely, customers arrive independently. 
Hence, the number of arrivals per unit time is expected to follow the Poisson distribution. 
Let us see how we could fit Poisson distribution to the given data.  

```{r}
df_arrival <- read.csv("customers.csv")
str(df_arrival)

arrival <- df_arrival$arrival
arrival
```

```{r}
summary(arrival)  # maximum arrival time is 32.1 mins
hist(arrival)

# we use sapply to calculate the number of arrivals per minute
y <- sapply(1:33, function(i) sum(arrival >= i-1 & arrival < i))
y

hist(y)

table(y)

tabulate(y)
```

So, there are 16 minutes with no arrival at all, 12 minutes with exactly 1 arrival (per minute), 4 minutes with 2 arrivals and 1 minute with 3 arrivals.

## 3-2: Fitting a Discrete Variable

```{r}
arrival_pois <- fitdist(y, "pois")
arrival_pois
(lambda <- arrival_pois$estimate)

mean(y)
```

This is not a coincidence as the mean of a Poisson distribution is equal to its lambda value. 

Here, the lambda value is close to 0.70. It indicates that on average, there are 7 new customers per 10 minutes. 

## 3-3: Evaluation of the Fit's Quality

### Rootograms

A rootogram shows if a discrete (or count) variable is well-fitted to a discrete distribution. It compares the fitted count values to the observed ones.

```{r}
# observed frequency:
(observed <- table(y))

# Alternatively,
observed <- c(table(y)[1], tabulate(y))
names(observed) <- 0:max(y)
observed

# expected/fitted frequency:
fitted_pois <- dpois(x = 0:max(y), lambda = lambda) * sum(observed)
fitted_pois

rootogram(x = observed,
          fitted = fitted_pois,
          type = "hanging")
```

Just like the continuous variable case, we can also use the `denscomp()` function to compare the fitted probability mass function (pmf) to the observed count values.

```{r}
denscomp(arrival_pois)
```

This chart overlays three graphics:

1. The histogram of the data.
2. The fitted Poisson pmf in red.
3. The empirical pmf of the observed data in black. 

From the rootogram and the denscomp plot, the number of arrivals per minute data fits Poisson distribution well. 

By the independence of the customers' arrival, the inter-arrival time also follows the Exponential distribution.
Let us first prepare the data by calculating the inter-arrival time sequence. 

```{r}
arrival

# Calculate the inter-arrival time vector
inter_arrival <- arrival[2:23] - arrival[1:22]
inter_arrival

hist(inter_arrival)

arrival_exp <- fitdist(inter_arrival, "exp")
arrival_exp

qqcomp(arrival_exp)
denscomp(arrival_exp)
```

From the Q-Q plot, the inter-arrival time data is relatively well fitted by Exponential distribution. The key parameter, rate, is estimated to be 0.6962, which is very close to the lambda value of the Poisson distribution.

Suppose the rate (for both Exponential and Poisson distribution) is 4. We will expect 4 new customers arriving every minute, and the expected mean of the inter-arrival time is 1/4 minute.  

# Summary/Key Takeaways

1. Sampling distribution may not be normal, especially when the sample size is small or when the population data do not follow normal distribution. 
2. Bootstrap and permutation tests are preferred if you wish to use a new test statistic, or if the sample size is not large.
3. Poisson distribution and Exponential distribution describe different aspects of the Poisson process (e.g., customers' arrival sequences).

# References 

1. Chihara, L. & Hesterberg, T. (2019).  Mathematical statistics with resampling in R, Chapter 3 & 5.
2. Mine Çetinkaya-Rundel and Johanna Hardin. Introduction to Modern Statistics, Chapter 11, 17 & 20.
3. [Approximate is Better than Exact, Agresti and Coull](http://cox.csueastbay.edu/~esuess/classes_old/Statistics_65016502/Handouts/2013/6502/papers/p.hat/agresti&coull.pdf), The American Statistician (1998).

# Appendix

## Use `boot` library to generate the non-parametric bootstrap confidence intervals 

```{r}
library(boot)

# wrong way
bootstrap <- boot(fees_AI, mean, R = 10000)

# Define function to take in an index i as a second argument
meanfun <- function(data, i){
  d <- data[i, ] # allows boot to select sample
  return(mean(d, na.rm = TRUE))   
}

set.seed(1) # for reproducibility, i.e., the results would be the same for anyone who run the same code

boot_fees_AI2 <- boot(as.data.frame(fees_AI),
                    statistic=meanfun, 
                    R=10000)
boot_fees_AI2

boot.ci(boot.out = boot_fees_AI2, type = c("norm", "basic", "perc"))

# compare with the earlier CIs
print(paste0("The 95% classical SE CI for mean is ", ci_classical))
print(paste0("The 95% Bootstrap SE CI for mean is ", ci_boot_se))
print(paste0("The 95% Bootstrap Percentile CI for mean is ", formatCI(ci_boot_perc)))
```

Why the numbers are still different, even after using the same seed?

## Fitting a Continuous Variable

To fit some distribution to a continuous variable (stored as a vector), there are two steps:

1.   fit_object <- fitdist(data = *a numeric vector*, distr = "*distr type*")
2.   qqcomp(fit_object)    # for generating the Q-Q plot

	   denscomp(fit_object) # for overlaying the fit pdf on the histogram

The `fitdistrplus` package provides routines to find optimal parameters for the 
following common named distributions, and more:

* [Normal](https://en.wikipedia.org/wiki/Normal_distribution) `norm`
* [Log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution) `lnorm`
* [Exponential](https://en.wikipedia.org/wiki/Exponential_distribution) `exp`
* [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution) `pois`
* [Cauchy](https://en.wikipedia.org/wiki/Cauchy_distribution) `cauchy`
* [Gamma](https://en.wikipedia.org/wiki/Gamma_distribution) `gamma`
* [Logistic](https://en.wikipedia.org/wiki/Generalized_logistic_distribution) `logis`
* [Negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) `nbinom`
* [Geometric](https://en.wikipedia.org/wiki/Geometric_distribution) `geom`
* [Beta](https://en.wikipedia.org/wiki/Beta_distribution) `beta`
* [Weibull](https://en.wikipedia.org/wiki/Weibull_distribution) `weibull`


## Generating Random Values 

Suppose the distribution fits well to the data, and we have stored the parameter(s) of the distribution. 

As the end target is to simulate future events, the next step is to generate random values (variables) following the "named" distribution. In R, we can use functions that begin with `r`. For instance,

* `rnorm` generates a vector of normally distributed numbers. Precisely speaking, it generates a sequence of independent and identically distributed random variables following a Normal distribution.
* `rexp` generates a vector of exponentially distributed numbers.
* `rgamma` generates a vector from a Gamma distribution, and so on.

If you are not sure what parameter is involved, say, for Exponential distribution, you can use `?rexp`. 

```{r}
X <- rnorm(100, mean=10, sd=4)
summary(X)
sd(X)
hist(X)
```

As you can see, we all get different answers; different histograms. 

```{r}
set.seed(13)
X <- rnorm(100, mean=10, sd=4)
summary(X)
sd(X)
hist(X)
```

`set.seed()` initialises the random number generator in R, so that the stream of random numbers is reproduced. When we simulate, it is useful to be able to do this for a couple of reasons:

1. Reproducibility.
2. If we vary the seed and still observe similar outcomes, we can be more confident in our conclusions.
3. Sometimes, we can use it to de-bug the error(s).

Another function we use to generate random events, is `sample`. 

```{r}
set.seed(10)
sample(10, size = 3) # choose 3 objects, without replacement from 1 to 10
sample(5:10, size=3) # choose 3 objects, without replacement from 5,6,7,8,9,10
sample(1:2, size=5, replace=TRUE) # choose 5 objects, with replacement from 1 and 2.
```

You might notice that each number in the pool has an equal chance of being selected. In practice, we very often need to do a non-equal probability lucky draw or simulation. 

For example, there are many customers' arrival at the entrance of a bank's branch. They will approach the queue ticket dispenser, select their business type, and take a queue ticket. For convenience, assume that 40% of customers have account related business (account opening or inquiries), 30% have cash related business (deposits, withdrawals, etc), and the remaining 30% have other types of business. 

If our target is to generate a sequence of numbers indicating the type of business for each customer, this could be simulated in the following way.

```{r}
set.seed(123)
entity_type <- sample(1:3, size = 100, replace = TRUE, 
                      prob = c(0.4, 0.3, 0.3)) 
entity_type

table(entity_type)
```

You could try to delete the seed part and re-run it repeatedly. 

## Using the Fitted Distribution to Run Simulations

Suppose that the estimated lambda (and rate) is 0.7, namely, there are 7 arrivals per 10 minutes on average. Our task is to generate a sequence of arrival times for another 20 new customers.   

```{r}
lambda <- 0.7 # 0.7 arrivals per minute
rate   <- 0.7 # it also means 0.7 arrivals per minute

set.seed(1)
rpois(20, lambda = lambda) # it is not helpful for a discrete event simulation

set.seed(1)
# inter-arrival time sequences
X <- rexp(20, rate = rate)
X

cumsum(X) # the sequence of arrival times for another 20 new customers. 
```

Half of the course is about running simulation and applying simulation. Simulating involves the generation of random variables. It is often conducted in a iterative and repeated way. The analysis results will also be averaged. The law of large numbers guarantees us that 
the sample average will be close to the mean of the distribution that the numbers came from. 

For example, the following code is to simulate samples in order to estimate the mean of the standard normal distribution with mean = 0, sd = 1. 

```{r}
X <- rnorm(10)
mean(X)

X <- rnorm(10000)
mean(X)
```

As the sample size or the number of replications increases, the sample mean approaches the true population mean more closely.

Here, the main takeaway is that once the simulation model is established, it will be run repeatedly!

### Monte Carlo Method for estimating Pi

```{r}
set.seed(123)
size <- 1000000
count <- 0

for (i in 1:size) {
  x <- runif(1, -1, 1)
  y <- runif(1, -1, 1)
  if (x^2 + y^2 <= 1) {
    count <- count + 1
  }
}

count/size * 4
```

Alternatively,

```{r}
set.seed(123)
size <- 1000000

x <- runif(size, -1, 1)
y <- runif(size, -1, 1)
count <- sum(x^2 + y^2 <= 1)

count/size * 4
```